--------------------------------------------------------------------------------------------------------
-- Redshift
--------------------------------------------------------------------------------------------------------
■ S3 <-> Redshift

★TSVは、UTF-8 LF

○デリミタ = タブの場合
UNLOAD( 'SELECT * FROM schema_name.TARGET_TABLE' )
    TO 's3://S3パス/TARGET_TABLE.tsv'
    CREDENTIALS 'aws_access_key_id=XXXXX;aws_secret_access_key=XXXXX'
    DELIMITER '\t' ESCAPE GZIP ALLOWOVERWRITE;
 ↓
COPY schema_name.TARGET_TABLE FROM 's3://S3パス/TARGET_TABLE.tsv'
    CREDENTIALS 'aws_access_key_id=XXXXX;aws_secret_access_key=XXXXX'
    DELIMITER '\t' ESCAPE GZIP DATEFORMAT 'auto';


○デリミタ = カンマ、ダブルクォーテーションの場合
UNLOAD( 'SELECT * FROM schema_name.TARGET_TABLE' )
    TO 's3://S3パス/TARGET_TABLE.csv'
    CREDENTIALS 'aws_access_key_id=XXXXX;aws_secret_access_key=XXXXX'
    DELIMITER ',' GZIP ADDQUOTES ALLOWOVERWRITE;
 ↓
COPY schema_name.TARGET_TABLE FROM 's3://S3パス/TARGET_TABLE.csv'
    CREDENTIALS 'aws_access_key_id=XXXXX;aws_secret_access_key=XXXXX'
    GZIP DELIMITER ',' REMOVEQUOTES DATEFORMAT 'auto' IGNOREHEADER 1;


  ADDQUOTES             ダブルクォーテーションをつける  → 読み込む際は、REMOVEQUOTES  が必要
  ESCAPE                区切り文字が含まれる場合にエスケープされる。読み込む際も必要
  ALLOWOVERWRITE        S3ファイル上書きする
  DELIMITER ','         DELIMITER '\t'
  PARALLEL OFF          通常(省略)時はON。 OFFの場合、6.2GBまで1つのファイルに出力する。時間がかかる
  IGNOREHEADER 1        ヘッダ行あり(1行)
  DATEFORMAT 'auto'     日付フォーマット
  TIMEFORMAT 'auto'     時間フォーマット

  ※ファイル名は、大文字、小文字を区別する
  ※改行コードは、\n にする
  ※文字コードは、UTF-8にする
  ※CSVからのコピー時、CSVのパラメータを外して、REMOVEQUOTESを追加すれば成功する場合がある


■ SQLファイル実行

psql -h XXXXX.redshift.amazonaws.com -U db_user1 -d db_name1 -p 1234 -f ./test.sql > test.log &



■Redshift SQL引数渡し

psql -h XXXX.amazonaws.com -U ユーザ -d DB名 -p ポート -f 
  test.sql -v SCHEMA=schema_name -v TBL=table_name -v S3FILE="'s3://S3パス/ファイル名'"

test.sql
--------------------------------------
TRUNCATE TABLE :SCHEMA.:TBL;
COPY :SCHEMA.:TBL
FROM :S3FILE gzip credentials 'aws_access_key_id=XXXXX;aws_secret_access_key=XXXXX'  REMOVEQUOTES DELIMITER ',' IGNOREHEADER 1;
COMMIT;
--------------------------------------


■ Oracle → Redshift テーブル定義変換

numeric          (decimal)  小数点前までは131,072桁、小数点以降は16,383桁
smallint         (int2)      5桁                    -32,768 ～                    +32,767
integer          (int,int4) 10桁             -2,147,483,648 ～             +2,147,483,647
bigint           (int8)     19桁 -9,223,372,036,854,775,808 ～ +9,223,372,036,854,775,807
real             (float4)    6桁精度
double precision (float8)   15桁精度



■テーブル作成(元をコピー:圧縮エンコード含む)

create table new_table (like  src_table); 


■圧縮エンコード
http://docs.aws.amazon.com/ja_jp/redshift/latest/dg/r_ANALYZE_COMPRESSION.html

1. analyze compression テーブル名
2. unload
3. drop table
4. create new table
5. load


■ 時間差の計算
select
    tbl_name
  , start_time
  , finish_time
  , to_char(trunc(mod(date_diff('sec', start_time, finish_time)/60/60, 60)), '00') || ':' ||
    to_char(trunc(mod(date_diff('sec', start_time, finish_time)/60, 60)), '00')    || ':' ||
    to_char(mod(date_diff('sec', start_time, finish_time), 60), '00')              as elapsed
from  ログテーブル
order by
    tbl_name
  , start_time;


■ テーブル権限

GRANT SELECT ON schema_1.table_a TO abc_developer;



--------------------------------------------------------------------------------------------------------
-- EC2, Linux
--------------------------------------------------------------------------------------------------------
■ ファイル一覧、検索

S3 検索
aws s3 ls s3://S3パス/ --recursive | grep 'SEARCH_WORD' > result.txt


grep
-i 大文字小文字区別しない
-E 正規表現
-a テキストファイルとして検索(バイナリではなく)


S3 リスト作成
aws s3 ls s3://S3パス/ --recursive --human-readable --summarize > s3list.txt


EC2 検索
find /home/user1 -type f -iregex ".*SEARCH_WORD.*" | xargs ls -l --time-style=+%Y-%m-%d\ %H:%M:%S > result.txt

find /home/user1/aaaa/ -type f -iregex ".*SEARCH_WORD.*" | xargs ls -l --time-style=+%Y-%m-%d\ %H:%M:%S > result.txt



-iregex  大文字小文字区別しない 正規表現

xargs 実行



■ S3 <-> EC2 コピー

ローカルからS3にコピー
aws s3 cp <LocalPath> <S3Path>

S3からローカルにコピー
aws s3 cp <S3Path> <LocalPath>

S3からS3にコピー
aws s3 cp <S3Path> <S3Path>

キー情報が必要な場合
AWS_ACCESS_KEY_ID=xxxx AWS_SECRET_ACCESS_KEY=xxxx aws s3 cp test.txt s3://my-bucket/


■SQLエラー時の戻り値セット
WHENEVER OSERROR EXIT 2;
WHENEVER SQLERROR EXIT 1;

sqlplusでエラー  ->  sqlret にエラーメッセージ  ただし、ret=0
SQLがエラー      ->  ret>0 かつ csvtail にエラーメッセージ


■ ファイルの最終行を削除する
$ tail -n4 data.csv
ERROR:
ORA-00028: セッションは強制終了されました。


$ head -n -4 data.csv > data_new.csv


■ ファイルの行数カウント

wc -l ファイル名


■ 1か月前のファイルを削除する

find ./hoge/log -name '*.log' -mtime +30 -delete

find ./hoge/log -name '*.log' -mtime +30 | xargs -r rm

find ./hoge -type d -mtime +30 -print | xargs -r rm -rf
↑ -printは最後でないとダメ


■ 日付

# 日本時間
date -u -d '9 hours' '+%Y-%m-%d %H:%M:%S'

# 日本時間で3日前
date -u -d '9 hours -3 day' '+%Y%m%d'

# 3日前
date -d '-3 day'

# 指定フォーマットで表示
date +"%Y/%m/%d %p %H:%M:%S"


■ バックグラウンドのプロセスをキル

psの結果をsedで整形しPIDを取得して、それをkill

ps | grep pty0 | sed -E 's/ +/ /g' | cut -d' ' -f2 | xargs kill -9
ps | grep pty0 | sed -E 's/ +/ /g' | cut -d' ' -f2 | xargs -I {} sh -c 'kill -9 {}'


過去ログを削除
find ./log -maxdepth 1 -name '*.log' -mtime +15 -print | xargs -I {} sh -c 'gzip {} && mv {}.gz ./log/_old/'

SJISをUTF変換しながらtail
tail -f lev_log.txt | while read LINE ; do echo $LINE | iconv -f SJIS -t UTF-8 ; done


